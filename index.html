<!DOCTYPE html>
<html lang='en'>

<head>
    <base href=".">
    <link rel="shortcut icon" type="image/png" href="assets/favicon.png"/>
    <link rel="stylesheet" type="text/css" media="all" href="assets/main.css"/>
    <meta name="description" content="Multimodal Representation and Retrieval Workshop at ICCV 2025">
    <meta name="resource-type" content="document">
    <meta name="distribution" content="global">
    <meta name="KeyWords" content="Conference, Workshop, Multimodal, Representation, Retrieval, ICCV">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://mrr-workshop.github.io/">
    <meta property="og:title" content="Multimodal Representation and Retrieval [MRR 2025]">
    <meta property="og:description" content="A workshop on multimodal representation and retrieval at ICCV 2025, focusing on multimodal data in applications like e-commerce, social media, and short videos.">
    <meta property="og:image" content="https://mrr-workshop.github.io/assets/pexels-lastly-412681-half.jpg">
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://mrr-workshop.github.io/">
    <meta property="twitter:title" content="Multimodal Representation and Retrieval [MRR 2025]">
    <meta property="twitter:description" content="A workshop on multimodal representation and retrieval at ICCV 2025, focusing on multimodal data in applications like e-commerce, social media, and short videos.">
    <meta property="twitter:image" content="https://mrr-workshop.github.io/assets/pexels-lastly-412681-half.jpg">
    <style>
        .profile-image {
            width: 100px;
            height: 100px;
            border-radius: 50%;
            object-fit: cover;
            margin-right: 10px;
            vertical-align: middle;
        }
        .profile-link {
            display: flex;
            align-items: center;
            text-decoration: none;
            color: black;
        }
        @media (max-width: 600px) {
            .profile-link {
                flex-direction: column;
                align-items: flex-start;
            }
            .profile-image {
                margin-bottom: 10px;
            }
        }
    </style>
    <style>
        .navigation {
            width: 100%;
            text-align: center;
            border-collapse: collapse;
        }
        .navigation td {
            padding: 10px 20px; /* Adjust padding as needed */
        }
        .navigation a {
            text-decoration: none;
            color: black;
            font-size: 16px;
            font-weight: bold;
        }
        .navigation a:hover {
            color: goldenrod;
        }
        .program-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        .program-table th, .program-table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        .program-table th {
            background-color: #f2f2f2;
            font-weight: bold;
        }
        .program-table .time-col {
            width: 15%;
            font-weight: bold;
        }
        .program-table .session-col {
            width: 85%;
        }
        .keynote-row {
            background-color: #f9f9f9;
            font-weight: bold;
        }
        .break-row {
            background-color: #e8f4f8;
            font-style: italic;
        }
    </style>
    <title>Multimodal Representation and Retrieval [MRR 2025]</title>
</head>


<body>

    <a id="home"><div class="banner">
        <img src="assets/pexels-lastly-412681-half.jpg" alt="Conference Template Banner" width="1000px">
        <div class="top-center">
            <span class="title1">Multimodal Representation and Retrieval</span><br />
        </div>
	<div class="top-left">
	  <span class="title2"></span> <span class="year">MRR 2025</span>
	</div>
        <div class="bottom-right">
            8:30am - 12:30pm, October 20, 2025 @ ICCV <br> Honolulu, Hawai'i
        </div>
    </div></a>

    <table class="navigation">
        <tr>
            <td class="navigation">
                <a class="current" title="Conference Home Page" href="index.html#">Home</a>
            </td>
            <td class="navigation">
                <a title="Register for the Conference" href="https://iccv.thecvf.com/Conferences/2025/">Registration (ICCV 2025)</a>
            </td>
            <td class="navigation">
                <a title="Workshop Program" href="index.html#program">Program</a>
            </td>
            <td class="navigation">
                <a title="Keynote Speakers" href="index.html#keynote_speakers">Keynote Speakers</a>
            </td>
            <td class="navigation">
                <a title="Accepted Papers" href="index.html#accepted_papers">Accepted Papers</a>
            </td>
            <td class="navigation">
                <a title="Organizers" href="index.html#organizers">Organizers</a>
            </td>
            <td class="navigation">
                <a title="Past Events" href="index.html#past_events">Past Events</a> 
            </td>
        </tr>
    </table>

    <h2>Abstract</h2>
    <p style="margin: 0; padding: 0;">
Multimodal representation learning is central to modern AI, enabling applications across retrieval, generation, RAG, reasoning, agentic AI, and embodied intelligence. With the growing ubiquity of multimodal data—from e-commerce listings to social media and video content—new challenges arise in multimodal retrieval, where both queries and indexed content span multiple modalities. This task requires deeper semantic understanding and reasoning, especially at scale, where data complexity and noise become significant hurdles. Following the success of our first edition, the second Multimodal Representation and Retrieval Workshop at ICCV 2025 will continue to foster progress in this critical area. The half-day event will feature keynote talks, an invited talk, and oral and poster presentations.
    </p>


    <a id="cfp"><h2>Call for Papers</h2></a>
    <br />
    <p style="margin: 0; padding: 0;">
    Our objective with this workshop is to capture the interest of researchers in the emerging field of multimodal retrieval and representation learning. As users increasingly use LLM based agents to interact with the world, the tools needed to retrieve relevant information will need to evolve to serve agents as well as human users. We anticipate that the workshop will serve as a catalyst for establishing a dedicated community focused on this topic. By highlighting the novelty and significance of the problem, we aim to attract researchers who are eager to explore and contribute to this field. We invite original research & industrial application papers that present research on learning multimodal representations and building multimodal retrieval systems.
    </p>

    <section id="submission-guidelines">
        <h3>Submission Guidelines</h3>
        <p style="margin: 0; padding: 0;">Submissions of papers must be in English, in PDF format, and at most 8 pages (including figures, tables, proofs, appendixes, acknowledgments, and any content except references) in length, with unrestricted space for references, in the ICCV style. Please download the <a href="https://media.eventhosts.cc/Conferences/ICCV2025/ICCV2025-Author-Kit-Feb.zip">ICCV 2025 Author Kit</a> for detailed formatting instructions.</p>
        <br/>
        <p style="margin: 0; padding: 0;">Papers that are not properly anonymized, or do not use the template, or have less than four pages or more than eight pages (excluding references) will be rejected without review. We expect at least one author from each submission to be available as a reviewer.</p>
        <br/>
        <p style="margin: 0; padding: 0;">Submissions should be submitted electronically:</p>
        <br/>
        <p><a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/MRR">Submit via OpenReview</a></p>
        <br/>
        <p style="margin: 0; padding: 0;">The accepted papers will appear in ICCV proceedings by default unless the authors notify the organizers (email: mrr-2025-iccv@googlegroups.com
) separately before Jul 3 (11:59 pm, PST).</p>
    
        <h3>Important dates for submissions to MRR 2025</h3>
        <ul class="custom-bullets">
            <li>Workshop paper submission due date: <strike>June 3</strike> June 10, 2025 (11:59 pm, AoE)</li>
            <li>Workshop paper acceptance notification: <strike>June 25</strike> July 2, 2025</li>
            <li>Workshop day: 8:30am - 12:30pm, October 20 (morning), 2025</li>
            <li>Location: 308B</li>
        </ul>
        
        <section id="topics">
            <h3>Topics includes but not limited to</h3>
            <ul class="custom-bullets">
                <li><strong>Multimodal representation learning and retrieval, such as</strong>
                    <ul class="custom-sub-bullets">
                        <li>Multimodal embeddings learning and fusion</li>
                        <li>Multimodal representation for reasoning</li>
                        <li>Learning with noisy labels</li>
                        <li>Multimodal query representation</li>
                        <li>Multimodal query understanding</li>
                        <li>Multimodal query suggestion</li>
                        <li>Ranking algorithms for multimodal retrieval</li>
                    </ul>
                </li>

                <li><strong>Dataset, such as</strong>
                    <ul class="custom-sub-bullets">
                        <li>New dataset for multimodal representation/reasoning/retrieval</li>
                        <li>Ways to synthesize data</li>
                    </ul>
                </li>

                <li><strong>Applications of Multimodal Retrieval, such as</strong>
                    <ul class="custom-sub-bullets">
                        <li>Multimodal retrieval in RAG</li>
                        <li>Multimodal retrieval in Agentic AI</li>
                        <li>Multimodal retrieval in search engine</li>
                        <li>Multimodal retrieval in recommendation system</li>
                        <li>Multimodal retrieval in Ads</li>
                        <li>Multimodal retrieval in Chatbot</li>
                        <li>Multimodal query suggestion</li>
                        <li>Multimodal retrieval in Robotics</li>
                    </ul>
                </li>
            </ul>
        </section>

    <a id="program"><h2>Program</h2></a>
    <table class="program-table">
        <thead>
            <tr>
                <th class="time-col">Time</th>
                <th class="session-col">Session</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td class="time-col">8:30 - 8:35 am</td>
                <td class="session-col">Opening Remarks</td>
            </tr>
            <tr>
                <td class="time-col">8:35 - 8:55 am</td>
                <td class="session-col">Invited Talk - Roei Herzig<br><i>Towards Structured Physical Intelligence Models</i></td>
            </tr>
            <tr class="keynote-row">
                <td class="time-col">8:55 - 9:35 am</td>
                <td class="session-col">Keynote: Cordelia Schmid<br><i>Multi-modal video understanding</i></td>
            </tr>
            <tr>
                <td class="time-col">9:35 - 9:50 am</td>
                <td class="session-col"><a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Chen_Rate-Distortion_Limits_for_Multimodal_Retrieval_Theory_Optimal_Codes_and_Finite-Sample_ICCVW_2025_paper.html">Rate–Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees</a><br><i>Thomas Y Chen</i></td>
            </tr>
            <tr class="break-row">
                <td class="time-col">9:50 - 10:35 am</td>
                <td class="session-col">Coffee Break & <a href="#accepted_papers">Poster Session</a></td>
            </tr>
            <tr>
                <td class="time-col">10:35 - 10:50 am</td>
                <td class="session-col"><a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Yu_MIND-RAG_Multimodal_Context-Aware_and_Intent-Aware_Retrieval-Augmented_Generation_for_Educational_Publications_ICCVW_2025_paper.html">MIND-RAG: Multimodal Context-Aware and Intent-Aware Retrieval-Augmented Generation for Educational Publications</a><br><i>Jiayang Yu, Yuxi Xie, Guixuan Zhang, Jie Liu, Zhi Zeng, Ying Huang, Shuwu Zhang</i></td>
            </tr>
            <tr class="keynote-row">
                <td class="time-col">10:50 - 11:30 am</td>
                <td class="session-col">Keynote: Jianwei Yang<br><i>Towards Intelligent Multimodal AI Agents: From Digital to Physical Worlds and Beyond</i></td>
            </tr>
            <tr>
                <td class="time-col">11:30 - 11:45 am</td>
                <td class="session-col"><a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Dai_Refining_Skewed_Perceptions_in_Vision-Language_Contrastive_Models_through_Visual_Representations_ICCVW_2025_paper.html">Refining Skewed Perceptions in Vision-Language Contrastive Models through Visual Representations</a><br><i>Haocheng Dai, Sarang Joshi</i></td>
            </tr>
            <tr class="keynote-row">
                <td class="time-col">11:45 - 12:25 pm</td>
                <td class="session-col">Keynote: Kristen Grauman<br><i>Multimodal activity understanding</i></td>
            </tr>
            <tr>
                <td class="time-col">12:25 - 12:30 pm</td>
                <td class="session-col">Closing Remarks</td>
            </tr>
        </tbody>
    </table>
        
    <a id="keynote_speakers"><h2>Keynote Speakers</h2></a>
    
    <h3>Cordelia Schmid</h3>
    <a class="profile-link" href="https://cordeliaschmid.github.io/">
        <img class="profile-image" src="./assets/Keynote_mrr25/Cordelia.jpg" alt="Cordelia Schmid"> Inria Research Director
    </a>
    <p style="margin: 12px 0 4px 0; padding: 0;"><strong>Title:</strong> Multi-modal video understanding</p>
    <p style="margin: 8px 0 0 0; padding: 0;"><strong>Bio:</strong> Cordelia Schmid holds a M.S. degree in Computer Science from the University of Karlsruhe and a Doctorate, also in Computer Science, from the Institut National Polytechnique de Grenoble (INPG). Her doctoral thesis on "Local Greyvalue Invariants for Image Matching and Retrieval" received the best thesis award from INPG in 1996. She received the Habilitation degree in 2001 for her thesis entitled "From Image Matching to Learning Visual Models". Dr. Schmid was a post-doctoral research assistant in the Robotics Research Group of Oxford University in 1996--1997. Since 1997 she has held a permanent research position at Inria, where she is a research director.</p>
    <p style="margin: 8px 0 24px 0; padding: 0;">Dr. Schmid is a member of the German National Academy of Sciences, Leopoldina and a fellow of IEEE and the ELLIS society. She was awarded the Longuet-Higgins prize in 2006, 2014 and 2016, the Koenderink prize in 2018 and the Helmholtz prize in 2023, all for fundamental contributions in computer vision that have withstood the test of time. She received an ERC advanced grant in 2013, the Humboldt research award in 2015, the Inria & French Academy of Science Grand Prix in 2016, the Royal Society Milner award in 2020 and the PAMI distinguished researcher award in 2021. In 2023 she received the Körber European Science Prize and in 2024 the European Inventor Award in the research category. Dr. Schmid has been an Associate Editor for IEEE PAMI (2001--2005) and for IJCV (2004--2012), an editor-in-chief for IJCV (2013--2018), a program chair of IEEE CVPR 2005 and ECCV 2012 as well as a general chair of IEEE CVPR 2015, ECCV 2020 and ICCV 2023. Starting 2018 she holds a joint appointment with Google research.</p>
    
    <h3>Kristen Grauman</h3>
    <a class="profile-link" href="https://www.cs.utexas.edu/~grauman/">
        <img class="profile-image" src="./assets/Keynote_mrr25/Kristen.jpg" alt="Kristen Grauman"> Professor, Department of Computer Science, UT Austin
    </a>
    <p style="margin: 12px 0 4px 0; padding: 0;"><strong>Title:</strong> Multimodal activity understanding</p>
    <p style="margin: 8px 0 0 0; padding: 0;"><strong>Bio:</strong> Kristen Grauman is a Professor in the Department of Computer Science at the University of Texas at Austin. Her research focuses on video understanding and embodied perception. Before joining UT-Austin in 2007, she received her Ph.D. at MIT. She is a AAAS Fellow, IEEE Fellow, AAAI Fellow, Sloan Fellow, and recipient of the 2025 Huang Prize and 2013 Computers and Thought Award. She and her collaborators have been recognized with several Best Paper awards in computer vision, including a 2011 Marr Prize and a 2017 Helmholtz Prize (test of time award). She has served as Associate Editor-in-Chief for PAMI and Program Chair of CVPR 2015, NeurIPS 2018, and ICCV 2023.</p>
    
    <h3>Jianwei Yang</h3>
    <a class="profile-link" href="https://jwyang.github.io/">
        <img class="profile-image" src="./assets/Keynote_mrr25/Jianwei.jpg" alt="Jianwei Yang"> Research Scientist, Meta MSL
    </a>
    <p style="margin: 12px 0 4px 0; padding: 0;"><strong>Title:</strong> Towards Intelligent Multimodal AI Agents: From Digital to Physical Worlds and Beyond</p>
    <p style="margin: 8px 0 24px 0; padding: 0;"><strong>Bio:</strong> Jianwei Yang is an AI Research Scientist at Meta, and previously a Principal Researcher at MSR. His research lies at the intersection of computer vision and multimodal learning, with a focus on developing general-purpose multimodal agents capable of interacting with both humans and environments. He has co-organized several academic events, including the Workshops on Transformers for Vision, Workshops on Computer Vision in the Wild, and Tutorials on Recent Advances in Vision Foundation Models. Jianwei has also served as an Area Chair for top-tier conferences such as ICCV, NeurIPS, and ICLR. His work has been recognized with several honors, including a Best Student Paper Finalist at CVPR 2022, first place in the V3Det Challenge at CVPR 2024, and the Best Paper Award at the CoRL 2024 LangRob Workshop.</p>

    <a id="accepted_papers"><h2>Accepted Papers</h2></a>
    <p>View all papers in the <a href="https://openaccess.thecvf.com/ICCV2025_workshops/MRR%202025">official proceedings</a>.</p>
    <ul class="custom-bullets">
        <li><strong>Poster 144:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Stein_Visual_Adaptive_Prompting_for_Compositional_Zero-Shot_Learning_ICCVW_2025_paper.html">Visual Adaptive Prompting for Compositional Zero-Shot Learning</a><br><em>Kyle Stein, Andrew Mahyari, Guillermo Francia, Eman El-Sheikh</em></li>
        <li><strong>Poster 145:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Dela_Rosa_Smart_Routing_for_Multimodal_Video_Retrieval_When_to_Search_What_ICCVW_2025_paper.html">Smart Routing for Multimodal Video Retrieval: When to Search What</a><br><em>Kevin Dela Rosa</em></li>
        <li><strong>Poster 146:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Aiger_Global-to-Local_or_Local-to-Global_Enhancing_Image_Retrieval_with_Efficient_Local_Search_ICCVW_2025_paper.html">Global-to-Local or Local-to-Global? Enhancing Image Retrieval with EfficientLocal Search and Effective Global Re-ranking</a><br><em>Dror Aiger, Bingyi Cao, Kaifeng Chen, Andre Araujo</em></li>
        <li><strong>Poster 147:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Zhao_IRR-LMM_Improving_On-demand_Retail_Recommendation_with_Large_Multi-Modal_Models_ICCVW_2025_paper.html">IRR-LMM: Improving On-demand Retail Recommendation with Large Multi-Modal Models</a><br><em>Yihao Zhao, Nan Lai, Xiaoming Li, Xu Yan, Wenhao Deng, Hujiang Huang, Shuai Zhang, Wei Lin</em></li>
        <li><strong>Poster 148:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Huybrechts_Document_Haystack_A_Long_Context_Multimodal_ImageDocument_Understanding_Vision_LLM_ICCVW_2025_paper.html">Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark</a><br><em>Goeric Huybrechts, Srikanth Ronanki, Sai Muralidhar Jayanthi, Jack FitzGerald, Srinivasan Veeravanallur</em></li>
        <li><strong>Poster 149:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Madavan_Med-GRIM_Enhanced_Zero-Shot_Medical_VQA_using_prompt-embedded_Multimodal_Graph_RAG_ICCVW_2025_paper.html">Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG</a><br><em>Rakesh Raj, Akshat Kaimal, Hashim Faisal, Chandrakala S</em></li>
        <li><strong>Poster 150:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Meinardus_Chrono_A_Simple_Blueprint_for_Representing_Time_in_MLLMs_ICCVW_2025_paper.html">Chrono: A Simple Blueprint for Representing Time in MLLMs</a><br><em>Boris Meinardus, Hector Garcia Rodriguez, Anil Batra, Anna Rohrbach, Marcus Rohrbach</em></li>
        <li><strong>Poster 151:</strong> Instructify: Demystifying Metadata to Visual Instruction Tuning Data Conversion<br><em>Jacob A Hansen, Wei Lin, Junmo Kang, Muhammad Jehanzeb Mirza, Hongyin Luo, Rogerio Feris, Alan Ritter, James R. Glass, Leonid Karlinsky</em></li>
        <li><strong>Poster 152:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Wu_Towards_Reporting_Bias_in_Visual-Language_Datasets_Bi-modal_Data_Augmentation_by_ICCVW_2025_paper.html">Towards Reporting Bias in Visual-language Datasets: Bi-modal DataAugmentation by Decoupling Object-attribute Association</a><br><em>Qiyu Wu, Mengjie Zhao, Yutong He, Lang Huang, Junya Ono, Hiromi Wakaki, Yuki Mitsufuji</em></li>
        <li><strong>Poster 153:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Dai_Refining_Skewed_Perceptions_in_Vision-Language_Contrastive_Models_through_Visual_Representations_ICCVW_2025_paper.html">Refining Skewed Perceptions in Vision-Language Contrastive Models through Visual Representations</a><br><em>Haocheng Dai, Sarang Joshi</em></li>
        <li><strong>Poster 154:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Yu_MIND-RAG_Multimodal_Context-Aware_and_Intent-Aware_Retrieval-Augmented_Generation_for_Educational_Publications_ICCVW_2025_paper.html">MIND-RAG: Multimodal Context-Aware and Intent-Aware Retrieval-Augmented Generation for Educational Publications</a><br><em>Jiayang Yu, Yuxi Xie, Guixuan Zhang, Jie Liu, Zhi Zeng, Ying Huang, Shuwu Zhang</em></li>
        <li><strong>Poster 155:</strong> <a href="https://openaccess.thecvf.com/content/ICCV2025W/MRR%202025/html/Chen_Rate-Distortion_Limits_for_Multimodal_Retrieval_Theory_Optimal_Codes_and_Finite-Sample_ICCVW_2025_paper.html">Rate–Distortion Limits for Multimodal Retrieval: Theory, Optimal Codes, and Finite-Sample Guarantees</a><br><em>Thomas Y Chen</em></li>
    </ul>

 <a id="organizers"><h2>Organizers</h2></a>
    <ul>
        <li>
            <a class="profile-link" href="https://www.linkedin.com/in/xinliang-zhu-6b589942/">
                <img class="profile-image" src="./assets/Organizer/xinliang.jpg" alt="Xinliang Zhu"> Xinliang Zhu, Amazon
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://www.linkedin.com/in/arnabdhua/">
                <img class="profile-image" src="./assets/Organizer/arnab.jpeg" alt="Arnab Dhua"> Arnab Dhua, Amazon
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://people.ucas.edu.cn/~shengshengqian?language=en">
                <img class="profile-image" src="./assets/Organizer/shengsheng.png" alt="Shengsheng Qian"> Shengsheng Qian, Associate Professor, Chinese Academy of Sciences 
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://eric-xw.github.io/">
                <img class="profile-image" src="./assets/Organizer/xin3.jpeg" alt="Xin (Eric) Wang"> Xin (Eric) Wang, Assistant Professor, University of California, Santa Cruz
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://www.grasp.upenn.edu/people/rene-vidal/">
                <img class="profile-image" src="./assets/Organizer/rvidal16.png" alt="Rene Vidal"> Rene Vidal, Rachleff University Professor, University of Pennsylvania
            </a>
        </li>
        <li>
            <a class="profile-link" href="https://www.linkedin.com/in/de3ug/">
                <img class="profile-image" src="./assets/Organizer/doug.jpeg" alt="Douglas Gray"> Douglas Gray, Amazon
            </a>
        </li>
    </ul>

    <a id="past_events"><h2>Past Events</h2></a>
    <ul class="custom-bullets">
        <li><a href="2024/">MRR 2024 @ SIGIR</a> - July 18, 2024, Washington DC</li>
    </ul>

    <section id="contact">
      <h2>Contact</h2>
      <p>
        For any questions, please email
        <a href="mailto:mrr-2025-iccv@googlegroups.com">
          mrr-2025-iccv@googlegroups.com
        </a>.
      </p>
    </section>
    
    <footer>
        &copy; MRR Workshop Organizers 2025
    </footer>
</body>
</html>
